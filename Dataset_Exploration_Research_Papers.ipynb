{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9rslr8L1o_T"
      },
      "source": [
        "# Research Paper Project \n",
        "The data set for this project consists of titles and abstracts for different research papers and corresponding classifications by general topic area. Possible topic areas are:\n",
        "* Computer Science\n",
        "* Physics\n",
        "* Mathematics\n",
        "* Statistics\n",
        "* Quantitative Biology\n",
        "* Quantitative Finance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q20-JFBr1p3e",
        "outputId": "01268f98-4c9a-4b4e-fbd0-7fdc305f81cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "research_papers.csv 100%[===================>]  22.08M  12.2MB/s    in 1.8s    \n"
          ]
        }
      ],
      "source": [
        "#@title Load your dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20%2B%20X/Group/Engineering/Research%20Paper/research_papers.csv\"\n",
        "research_data = pd.read_csv('research_papers.csv')\n",
        "research_data = research_data.drop(columns=[\"ID\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "vzhlX2Fn1vPy",
        "outputId": "9360e7fb-98fe-4b58-f2df-db75de120117"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   TITLE  \\\n",
              "0            Reconstructing Subject-Specific Effect Maps   \n",
              "1                     Rotation Invariance Neural Network   \n",
              "2      Spherical polyharmonics and Poisson kernels fo...   \n",
              "3      A finite element approximation for the stochas...   \n",
              "4      Comparative study of Discrete Wavelet Transfor...   \n",
              "...                                                  ...   \n",
              "20967  Contemporary machine learning: a guide for pra...   \n",
              "20968  Uniform diamond coatings on WC-Co hard alloy c...   \n",
              "20969  Analysing Soccer Games with Clustering and Con...   \n",
              "20970  On the Efficient Simulation of the Left-Tail o...   \n",
              "20971   Why optional stopping is a problem for Bayesians   \n",
              "\n",
              "                                                ABSTRACT  Computer Science  \\\n",
              "0        Predictive models allow subject-specific inf...                 1   \n",
              "1        Rotation invariance and translation invarian...                 1   \n",
              "2        We introduce and develop the notion of spher...                 0   \n",
              "3        The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
              "4        Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
              "...                                                  ...               ...   \n",
              "20967    Machine learning is finding increasingly bro...                 1   \n",
              "20968    Polycrystalline diamond coatings have been g...                 0   \n",
              "20969    We present a new approach for identifying si...                 1   \n",
              "20970    The sum of Log-normal variates is encountere...                 0   \n",
              "20971    Recently, optional stopping has been a subje...                 0   \n",
              "\n",
              "       Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
              "0            0            0           0                     0   \n",
              "1            0            0           0                     0   \n",
              "2            0            1           0                     0   \n",
              "3            0            1           0                     0   \n",
              "4            0            0           1                     0   \n",
              "...        ...          ...         ...                   ...   \n",
              "20967        1            0           0                     0   \n",
              "20968        1            0           0                     0   \n",
              "20969        0            0           0                     0   \n",
              "20970        0            1           1                     0   \n",
              "20971        0            1           1                     0   \n",
              "\n",
              "       Quantitative Finance  \n",
              "0                         0  \n",
              "1                         0  \n",
              "2                         0  \n",
              "3                         0  \n",
              "4                         0  \n",
              "...                     ...  \n",
              "20967                     0  \n",
              "20968                     0  \n",
              "20969                     0  \n",
              "20970                     0  \n",
              "20971                     0  \n",
              "\n",
              "[20972 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac546d3c-c19f-4b90-9a67-2453ca8f94d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20967</th>\n",
              "      <td>Contemporary machine learning: a guide for pra...</td>\n",
              "      <td>Machine learning is finding increasingly bro...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20968</th>\n",
              "      <td>Uniform diamond coatings on WC-Co hard alloy c...</td>\n",
              "      <td>Polycrystalline diamond coatings have been g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20969</th>\n",
              "      <td>Analysing Soccer Games with Clustering and Con...</td>\n",
              "      <td>We present a new approach for identifying si...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20970</th>\n",
              "      <td>On the Efficient Simulation of the Left-Tail o...</td>\n",
              "      <td>The sum of Log-normal variates is encountere...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20971</th>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subje...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20972 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac546d3c-c19f-4b90-9a67-2453ca8f94d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac546d3c-c19f-4b90-9a67-2453ca8f94d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac546d3c-c19f-4b90-9a67-2453ca8f94d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "research_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MItLXXvW1zEu",
        "outputId": "4e261e74-dfa5-46f5-93d4-52b7f234b91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16777,)\n",
            "(4195,)\n",
            "(16777,)\n",
            "(4195,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(research_data['ABSTRACT'], research_data['TITLE'], test_size=0.2, train_size=0.8, random_state=8)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "train_data = zip(X_train, y_train)\n",
        "val_data = zip(X_test, y_test)\n",
        "\n",
        "myFile = open('train_data.csv', 'w')\n",
        "writer = csv.writer(myFile)\n",
        "writer.writerow([\"abstract\", \"title\"])\n",
        "for tr in train_data:\n",
        "    writer.writerow(tr)\n",
        "myFile.close()\n",
        "\n",
        "myFile = open('validation_data.csv', 'w')\n",
        "writer = csv.writer(myFile)\n",
        "writer.writerow([\"abstract\", \"title\"])\n",
        "for tr in val_data:\n",
        "    writer.writerow(tr)\n",
        "myFile.close()"
      ],
      "metadata": {
        "id": "F3s5t3Tczs4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "predictions = open(\"generated_predictions2.txt\", \"r\")\n",
        "val2_data = zip(predictions, y_test)\n",
        "\n",
        "myFile = open('final_test2.csv', 'w')\n",
        "writer = csv.writer(myFile)\n",
        "writer.writerow([\"abstract\", \"title\"])\n",
        "for data in val2_data:\n",
        "    writer.writerow(data)\n",
        "myFile.close()\n"
      ],
      "metadata": {
        "id": "7bDiKt0v_tQZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFbEIVUL2ImH"
      },
      "source": [
        "# Dataset exploration and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "I5cW4CSp0v21",
        "outputId": "7939767e-fbc6-47bb-9f94-169ef9f4f077"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7caddca25c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ABSTRACT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# text = text.lower() # try adding this back in and see what happens!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstpwrds\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7caddca25c64>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ABSTRACT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# text = text.lower() # try adding this back in and see what happens!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstpwrds\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "stpwrds = stopwords.words('english')\n",
        "punctuation = string.punctuation\n",
        "\n",
        "# try changing to visualize more or less words\n",
        "num_words = 30\n",
        "\n",
        "# text cleaning maintaining all of our text as one string\n",
        "text = \" \".join(research_data['ABSTRACT'])\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(8,12));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Abstracts of Research Papers\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZIf4JemKBnr"
      },
      "outputs": [],
      "source": [
        "research_data['Computer Science'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOXp0Zkn2HMk"
      },
      "outputs": [],
      "source": [
        "categories = research_data[[\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]]\n",
        "cat_names = categories.columns\n",
        "categories = categories.transpose()\n",
        "cat_sums = categories.sum(axis=1)\n",
        "\n",
        "for i in range(len(cat_names)):\n",
        "  print(\"{cat_name} is the category of {times} research papers.\".format(cat_name=cat_names[i], times=str(cat_sums[i])))\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.pie(cat_sums, labels=cat_names, autopct='%.0f%%', shadow=True, startangle=90, radius=2, textprops={'size': 'larger'})\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6odaHRUn8ev0"
      },
      "outputs": [],
      "source": [
        "comp_science_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Computer Science\"] == 1)\n",
        "comp_science_abstracts = comp_science_abstracts.dropna()\n",
        "physics_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Physics\"] == 1)\n",
        "physics_abstracts = physics_abstracts.dropna()\n",
        "mathematics_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Mathematics\"] == 1)\n",
        "mathematics_abstracts = mathematics_abstracts.dropna()\n",
        "stats_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Statistics\"] == 1)\n",
        "stats_abstracts = stats_abstracts.dropna()\n",
        "biology_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Quantitative Biology\"] == 1)\n",
        "biology_abstracts = biology_abstracts.dropna()\n",
        "finance_abstracts = research_data[\"ABSTRACT\"].where(research_data[\"Quantitative Finance\"] == 1)\n",
        "finance_abstracts = finance_abstracts.dropna()\n",
        "\n",
        "num_words = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "35vup7hk_AY7"
      },
      "outputs": [],
      "source": [
        "#@title Computer Science\n",
        "text = \" \".join(comp_science_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Computer Science abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2R6QeUrm_BBl"
      },
      "outputs": [],
      "source": [
        "#@title Physics\n",
        "text = \" \".join(physics_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Physics abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j6sd1U4S_B0q"
      },
      "outputs": [],
      "source": [
        "#@title Mathematics\n",
        "text = \" \".join(mathematics_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Mathematic abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lBLsQz7k_DO1"
      },
      "outputs": [],
      "source": [
        "#@title Statistics\n",
        "text = \" \".join(stats_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Statistics Abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xJi5xh2K_D7C"
      },
      "outputs": [],
      "source": [
        "#@title Quantitative Biology\n",
        "text = \" \".join(biology_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Quantitative Biology abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5s6Ou44S_EjW"
      },
      "outputs": [],
      "source": [
        "#@title Quantitative Finance\n",
        "text = \" \".join(finance_abstracts)\n",
        "# text = text.lower() # try adding this back in and see what happens!\n",
        "text = \"\".join(_ for _ in text if _ not in punctuation)\n",
        "text = [t for t in text.split() if t not in stpwrds and not t.isdigit()]\n",
        "\n",
        "# We can use Counter to find the most frequent words in all our titles!\n",
        "words = [_[0] for _ in Counter(text).most_common(num_words)]\n",
        "frequency = [_[1] for _ in Counter(text).most_common(num_words)]\n",
        "\n",
        "# Making our plot look nice!\n",
        "plt.figure(figsize=(5,3));\n",
        "ax = sns.barplot(x=frequency, y=words)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.title(\"Most Frequent Keywords used in Quantitative Finance abstracts\");\n",
        "plt.xlabel(\"Frequency\", fontsize=14);\n",
        "plt.yticks(fontsize=14);\n",
        "plt.xticks(fontsize=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyqBEOB22jfS"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLWXP9RO0hnB",
        "outputId": "e6f4e8f6-1e91-47d5-8f8a-d7a6fa9279dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#@title Preprocessor\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def process_lang_data(text):\n",
        "  \n",
        "  cleaned_text = []\n",
        "  punctuation = string.punctuation\n",
        "  our_stopwords = stopwords.words('english')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  for token in word_tokenize(text):\n",
        "    if token not in punctuation and token not in our_stopwords:\n",
        "      lemmatized = lemmatizer.lemmatize(token)\n",
        "      cleaned_text.append(lemmatized)\n",
        "\n",
        "  return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqfVy_feJOVc",
        "outputId": "25cc5eea-092f-432b-970f-c1c4d111da18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  \"The parameter 'stop_words' will not be used\"\n"
          ]
        }
      ],
      "source": [
        "#@title BOW preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "bow = CountVectorizer(analyzer=process_lang_data, max_features=1000, stop_words='english', lowercase=True) # using the function we made above\n",
        "bow.fit(X_train)                             # fitting to our training data\n",
        "bow_train = bow.transform(X_train).toarray() # then transforming both training and testing data\n",
        "bow_test = bow.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "bXeKkw0GL2sN"
      },
      "outputs": [],
      "source": [
        "#@title TdifVectorizer preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer=process_lang_data, max_features=1000, lowercase=True)\n",
        "tfidf.fit(X_train)\n",
        "tfidf_train = tfidf.transform(X_train).toarray()\n",
        "tfidf_test = tfidf.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cjD8-KKMJfF",
        "outputId": "a6d5f609-b35d-45a1-d0c1-441a1da4e10c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ],
      "source": [
        "#@title Word 2 Vec preprocessing\n",
        "import gensim\n",
        "# have to pre-tokenize\n",
        "tokenize = research_data['ABSTRACT'].apply(word_tokenize)\n",
        "\n",
        "# take a look at the documentation to see what these parameters are changing!\n",
        "w2vec_model = gensim.models.Word2Vec(tokenize, min_count = 1, size = 10, window = 5, sg = 1)\n",
        "w2vec_model.train(tokenize, total_examples = len(research_data['ABSTRACT']),epochs=20)\n",
        "X_w2vec = w2vec_model[w2vec_model.wv.vocab]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B5IS0_vjYse",
        "outputId": "3bcc8df7-57f5-4ad8-bf28-77f7df50b2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3517581, 4257100)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#@title Continue to train Word 2 Vec model with titles too\n",
        "predictable_tokenized_titles = []\n",
        "for i in range(len(research_data)):\n",
        "  predictable_tokenized_titles.append(word_tokenize(research_data.iloc[i][0]))\n",
        "\n",
        "w2vec_model.build_vocab(predictable_tokenized_titles, update=True)\n",
        "w2vec_model.train(predictable_tokenized_titles, total_examples=w2vec_model.corpus_count, epochs=w2vec_model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "TwrNeHJOM-Lr"
      },
      "outputs": [],
      "source": [
        "#@title Pre-trained word 2 vec preprocessing\n",
        "import spacy\n",
        "from spacy.lang.en.examples import sentences \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "text_to_nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "def tokenize_vecs(text):\n",
        "    clean_tokens = []\n",
        "    for token in text_to_nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n",
        "          # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token)\n",
        "    return np.array(clean_tokens)\n",
        "\n",
        "def sum_and_avg_vectors():\n",
        "  '''\n",
        "    Returns the average of the embedding vectors\n",
        "  '''\n",
        "  tokenized_vectors = research_data[\"ABSTRACT\"].apply(tokenize_vecs)\n",
        "  return np.array(tokenized_vectors.apply(lambda x: np.sum([w.vector for w in x])/len(x)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvfbtQg9YtxM",
        "outputId": "1b4ef38f-21d2-4fb3-cef5-59b0f8d34009"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8eCFcL24abS"
      },
      "outputs": [],
      "source": [
        "w2vec_model.save(\"w2vec_trained2.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7MzX3mhb5Qry"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "imported_w2vec = gensim.models.Word2Vec.load(\"w2vec_trained2.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApoqjR_b1rfX"
      },
      "source": [
        "# Random Sampling\n",
        "This is a baseline model to sample frequent words with a random, but reasonable, title length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "E8HvSPGo2ALi"
      },
      "outputs": [],
      "source": [
        "def bow_sample(num_sample):\n",
        "  rand_sample = num_sample\n",
        "  indexes = []\n",
        "  for i in range(round(np.random.randint(6, 10))):\n",
        "    high_index = 0\n",
        "    for j in range(len(bow_test[rand_sample])):\n",
        "      if j in indexes:\n",
        "        continue\n",
        "      if bow_test[rand_sample][j] > bow_test[rand_sample][high_index]:\n",
        "        high_index = j\n",
        "    indexes.append(high_index)\n",
        "    high_index = 0\n",
        "  return indexes, rand_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i9EtNxPJX8S7"
      },
      "outputs": [],
      "source": [
        "def generate_bow_sampling_title(num_sample):\n",
        "  rand_sample_title_lst = []\n",
        "  indexes, rand_sample = bow_sample(num_sample)\n",
        "  sample = bow_test[rand_sample]\n",
        "  vocab = bow.vocabulary_\n",
        "  for i in indexes:\n",
        "    for j in vocab.items():\n",
        "      if i == j[1]:\n",
        "        rand_sample_title_lst.append(j[0])\n",
        "  rand_sample_title = \" \".join(rand_sample_title_lst)\n",
        "  return [y_test.iloc[num_sample], rand_sample_title]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LrGeZOScsX24",
        "outputId": "711c25a6-bb75-4d05-e3ec-56889b129afb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rate Optimal Binary Linear Locally Repairable Codes with Small Availability'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI3ATRPpbsXX",
        "outputId": "86fb21de-5b47-4b4b-e361-b84746c4a949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Rate Optimal Binary Linear Locally Repairable Codes with Small Availability', 'code binary linear rate associated class size 2 A']\n"
          ]
        }
      ],
      "source": [
        "bow_title = generate_bow_sampling_title(0)\n",
        "print(bow_title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "r0kgGQvXmYyE",
        "outputId": "33d21121-fb4e-4d60-b093-0b6c1c8a2948"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  A locally repairable code with availability has the property that every code\\nsymbol can be recovered from multiple, disjoint subsets of other symbols of\\nsmall size. In particular, a code symbol is said to have $(r,t)$-availability\\nif it can be recovered from $t$ disjoint subsets, each of size at most $r$. A\\ncode with availability is said to be 'rate-optimal', if its rate is maximum\\namong the class of codes with given locality, availability, and alphabet size.\\nThis paper focuses on rate-optimal binary, linear codes with small\\navailability, and makes four contributions. First, it establishes tight upper\\nbounds on the rate of binary linear codes with $(r,2)$ and $(2,3)$\\navailability. Second, it establishes a uniqueness result for binary\\nrate-optimal codes, showing that for certain classes of binary linear codes\\nwith $(r,2)$ and $(2,3)$-availability, any rate optimal code must be a direct\\nsum of shorter rate optimal codes. Third, it presents novel upper bounds on the\\nrates of binary linear codes with $(2,t)$ and $(r,3)$-availability. In\\nparticular, the main contribution here is a new method for bounding the number\\nof cosets of the dual of a code with availability, using its covering\\nproperties. Finally, it presents a class of locally repairable linear codes\\nassociated with convex polyhedra, focusing on the codes associated with the\\nPlatonic solids. It demonstrates that these codes are locally repairable with\\n$t = 2$, and that the codes associated with (geometric) dual polyhedra are\\n(coding theoretic) duals of each other.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_sim_scores = []\n",
        "for i in range(len(X_test)):\n",
        "  both_titles = generate_bow_sampling_title(i)\n",
        "  gen_title = text_to_nlp(both_titles[1])\n",
        "  actual_title = text_to_nlp(both_titles[0])\n",
        "  bow_sim_scores.append(actual_title.similarity(gen_title))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4WYt4ert38O",
        "outputId": "d92f732c-1bd2-4287-f9ab-7d111a023bea"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_similarity = sum(bow_sim_scores) / len(bow_sim_scores)\n",
        "bow_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDtIO2F_vU2I",
        "outputId": "4c5a8c2c-04f9-4ece-abda-f11d520a1906"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5435828548328143"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y15I9y0-bxyF"
      },
      "source": [
        "# Adjacency matrix of words\n",
        "This baseline model creates a matrix of adjacency of words, showing how much words appear next to each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1LcRahZ5fkjo"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix_words(num_sample):\n",
        "  rand_sample = num_sample#np.random.randint(len(X_test))\n",
        "  cleaned_text = process_lang_data(X_test.iloc[rand_sample])\n",
        "  tokenized_text = []\n",
        "  tokens = []\n",
        "  for i in cleaned_text:\n",
        "    token = i.lower()\n",
        "    tokenized_text.append(token)\n",
        "    if token not in tokens:\n",
        "      tokens.append(token)\n",
        "\n",
        "  adjacency_matrix = []\n",
        "  for i in range(len(tokens)):\n",
        "    adjacency_matrix.append([])\n",
        "\n",
        "  for i in range(len(adjacency_matrix)):\n",
        "    for j in range(len(tokens)):\n",
        "      adjacency_matrix[i].append(0)\n",
        "\n",
        "  for i in range(len(tokenized_text)-1):\n",
        "    for j in range(len(tokens)):\n",
        "      if tokenized_text[i] == tokens[j]:\n",
        "        for k in range(len(tokens)):\n",
        "          if tokenized_text[i-1] == tokens[k] and i != 0:\n",
        "            adjacency_matrix[j][k] += 1\n",
        "          if tokenized_text[i+1] == tokens[k]:\n",
        "            adjacency_matrix[j][k] += 1\n",
        "\n",
        "  df = pd.DataFrame(adjacency_matrix)\n",
        "  df.columns = tokens\n",
        "  df.index = tokens\n",
        "  return rand_sample, adjacency_matrix, df, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4ldWrlSJCINE"
      },
      "outputs": [],
      "source": [
        "def generate_title_adjacency_matrix(num_sample):\n",
        "  rand_sample, adjacency_matrix, adj_df, tokens = adjacency_matrix_words(num_sample)\n",
        "  sample = tfidf_test[rand_sample]\n",
        "  high_index = 0\n",
        "  for i in range(len(sample)):\n",
        "    if sample[i] >= sample[high_index]:\n",
        "      high_index = i\n",
        "\n",
        "  # find the most common word in the acjacency matrix\n",
        "  high_index_word = \"\"\n",
        "  vocab = tfidf.vocabulary_\n",
        "  for i in vocab.items():\n",
        "    if i[1] == sample[high_index]:\n",
        "      high_index_word = i[0]\n",
        "\n",
        "  matrix_index = 0\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == high_index_word:\n",
        "      matrix_index = i\n",
        "\n",
        "  title_words = []\n",
        "  already_used_indexes = []\n",
        "\n",
        "  for i in range(round(np.random.randint(6, 10))):\n",
        "    next_word_index = 0\n",
        "    for j in range(len(adjacency_matrix[matrix_index])):\n",
        "      if adjacency_matrix[matrix_index][j] >= adjacency_matrix[matrix_index][next_word_index] and j not in already_used_indexes:\n",
        "        next_word_index = j\n",
        "    title_words.append(tokens[next_word_index])\n",
        "    matrix_index = next_word_index\n",
        "    already_used_indexes.append(next_word_index)\n",
        "\n",
        "  gen_title = \" \".join(title_words)\n",
        "  return [y_test.iloc[rand_sample], gen_title]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHSyc9qEu5s",
        "outputId": "759f6d5a-4eba-432b-d4cd-2309454e0533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Rate Optimal Binary Linear Locally Repairable Codes with Small Availability', 'r -availability r,3 2 code linear']\n"
          ]
        }
      ],
      "source": [
        "title_adj_mtrx = generate_title_adjacency_matrix(0)\n",
        "print(title_adj_mtrx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjmatrix_sim_scores = []\n",
        "for i in range(len(X_test)):\n",
        "  both_titles = generate_title_adjacency_matrix(i)\n",
        "  gen_title = text_to_nlp(both_titles[1])\n",
        "  actual_title = text_to_nlp(both_titles[0])\n",
        "  adjmatrix_sim_scores.append(actual_title.similarity(gen_title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwK2r2LCHWYU",
        "outputId": "683f944b-048b-445d-a84f-698d57db099d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjacency_matrix_similarity = sum(adjmatrix_sim_scores) / len(adjmatrix_sim_scores)\n",
        "adjacency_matrix_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYdnFXILHsQ4",
        "outputId": "b57dd810-c3ea-494a-a21c-8e0ad79b7630"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6021719563735695"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtSza9RqI-BN"
      },
      "source": [
        "# Recurrent Neural Network (LSTM)\n",
        "Using word2vec to input the abstract into it and training it with word2vec titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "7JbRTcGfdoQ3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec_vocab_dict = dict({})\n",
        "for idx, key in enumerate(imported_w2vec.wv.vocab):\n",
        "    w2vec_vocab_dict[key] = imported_w2vec.wv[key]"
      ],
      "metadata": {
        "id": "zmmd8fwYXCuh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "42hbSGX4YOpL"
      },
      "outputs": [],
      "source": [
        "# Take predictable papers and add them to a new pd dataframe. Predictable papers have to have 150 words or more in the abstract\n",
        "predictable_tokenized_titles = []\n",
        "predictable_tokenized_abstracts = []\n",
        "for i in range(len(research_data)):\n",
        "  if len(word_tokenize(research_data.loc[i][1])) >= 150 and len(word_tokenize(research_data.loc[i][0])) >= 6:\n",
        "    predictable_tokenized_titles.append(word_tokenize(research_data.iloc[i][0]))\n",
        "    predictable_tokenized_abstracts.append(word_tokenize(research_data.loc[i][1]))\n",
        "\n",
        "predictable_papers = pd.DataFrame(list(zip(predictable_tokenized_titles, predictable_tokenized_abstracts)), columns=[\"Tokenized titles\", \"Tokenized abstracts\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictable_tokenized_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfzm4Wg5kS5b",
        "outputId": "9d4f168e-1529-452b-9ab0-541714b1635f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11435"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lVbHxPMPf6H1"
      },
      "outputs": [],
      "source": [
        "# Convert all predictable papers' abstracts and titles to their word 2 vec vectors. Abstract vector length are cut down to only 150 words\n",
        "titles_vectors = np.empty((11435, 6, 10))\n",
        "abstract_vectors = np.empty((11435, 150, 10))\n",
        "for i in range(len(predictable_papers)):\n",
        "  titl_vecs = np.empty((6, 10))\n",
        "  abst_vecs = np.empty((150, 10))\n",
        "  for j in range(len(predictable_papers.loc[i][0][:6])):\n",
        "    titl_vecs[j] = imported_w2vec.wv.get_vector(predictable_papers.loc[i][0][:6][j])\n",
        "  for j in range(len(predictable_papers.loc[i][1][:150])):\n",
        "    abst_vecs[j] = imported_w2vec.wv.get_vector(predictable_papers.loc[i][1][:150][j])\n",
        "  titles_vectors[i] = titl_vecs\n",
        "  abstract_vectors[i] = abst_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzsr8UrrVgU2",
        "outputId": "1ed37636-d4ef-4b04-d2dc-4feb0daae2e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11435"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(abstract_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "048tTpWgr4U4"
      },
      "outputs": [],
      "source": [
        "predictable_papers[\"Titles vecs\"] = titles_vectors.tolist()\n",
        "predictable_papers[\"Abstracts vecs\"] = abstract_vectors.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UxVHF1U0UrBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c717a3-4fa7-4b25-f5f8-ac8dd503da89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9148, 150, 10)\n",
            "(2287, 150, 10)\n",
            "(9148, 6, 10)\n",
            "(2287, 6, 10)\n"
          ]
        }
      ],
      "source": [
        "lstm_X_train, lstm_X_test, lstm_y_train, lstm_y_test = train_test_split(abstract_vectors, titles_vectors, test_size=0.2, train_size=0.8, random_state=8)\n",
        "print(lstm_X_train.shape)\n",
        "print(lstm_X_test.shape)\n",
        "print(lstm_y_train.shape)\n",
        "print(lstm_y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_uDo-ufeVhjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27282fc3-799e-4c47-d08b-35d397607a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 150, 10)           840       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 150, 10)          40        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 6, 250)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6, 10)             2510      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,390\n",
            "Trainable params: 3,370\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(units=10, input_shape=(150, 10), return_sequences=True)) # (*, 10)\n",
        "model.add(layers.BatchNormalization())\n",
        "# model.add(layers.AveragePooling2D(pool_size=(25, 1), strides=25, padding='same'))\n",
        "model.add(layers.Reshape((6, 250), input_shape=(150, 10)))\n",
        "#  model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))\n",
        "model.add(layers.Dense(units=10))\n",
        "model.compile(\n",
        "    loss=keras.losses.MeanAbsoluteError(),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    lstm_X_train, lstm_y_train, batch_size=64, epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWJ1imMgb_Dn",
        "outputId": "8bdbb727-f234-4356-aeaa-7d1633478d43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 7s 9ms/step - loss: 1.0044 - accuracy: 0.1257\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.7651 - accuracy: 0.2611\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.6166 - accuracy: 0.4863\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.5356 - accuracy: 0.6448\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 0.4987 - accuracy: 0.7018\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.4792 - accuracy: 0.7270\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.4676 - accuracy: 0.7389\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.4603 - accuracy: 0.7478\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.4553 - accuracy: 0.7523\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 0.4517 - accuracy: 0.7542\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac61456310>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_y_predict = model.predict(lstm_X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvIrI2jFtCa_",
        "outputId": "6c95e8ac-b63b-42a6-adcb-60211b99966c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 1s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_y_predict.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9z7tvvyw0qd",
        "outputId": "2111b376-4f9b-4493-e751-97d78bc31477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2287, 6, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "IosY1dyaxuXa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(A, B):\n",
        "  avg_a = 0\n",
        "  avg_b = 0\n",
        "  counter = 0\n",
        "  for i in range(len(A)):\n",
        "    avg_a += A[i]\n",
        "    avg_b += B[i]\n",
        "    counter += 1\n",
        "  avg_a /= counter\n",
        "  avg_b /= counter\n",
        "  cosine = np.dot(avg_a,avg_b)/(norm(avg_a)*norm(avg_b))\n",
        "  return cosine\n",
        "\n",
        "def cosine_similarity(list_1, list_2):\n",
        "  cos_sim = dot(list_1, list_2) / (norm(list_1) * norm(list_2))\n",
        "  return cos_sim"
      ],
      "metadata": {
        "id": "LrWVDkbPWmyN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_sim_scores = []\n",
        "for i in range(len(lstm_y_test)):\n",
        "  lstm_sim_scores.append(cos_sim(lstm_y_test[i], lstm_y_predict[i]))"
      ],
      "metadata": {
        "id": "xkm9IZJoa5R4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_similarity_score = sum(lstm_sim_scores) / len(lstm_sim_scores)\n",
        "lstm_similarity_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB4k8bp1Xg8A",
        "outputId": "110a1695-ede3-43c4-f4bc-0292d0869c91"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9032022955504698"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 summarizer performance"
      ],
      "metadata": {
        "id": "_6172VZZaf1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "predictions = open(\"generated_predictions.txt\", \"r\")"
      ],
      "metadata": {
        "id": "b1vL9LSfnEmh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst_predictions = predictions.readlines()"
      ],
      "metadata": {
        "id": "mmjgQ0RGnW8l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer_sim_scores = []\n",
        "for i in range(len(y_test)):\n",
        "  gen_title = text_to_nlp(lst_predictions[i])\n",
        "  actual_title = text_to_nlp(y_test.iloc[i])\n",
        "  summarizer_sim_scores.append(actual_title.similarity(gen_title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmGQ1k6joA_e",
        "outputId": "a2486d3b-3607-4c06-e5f2-374b6c4a5f24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer_similarity = sum(summarizer_sim_scores) / len(summarizer_sim_scores)\n",
        "summarizer_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijA0o66youAv",
        "outputId": "c0736a5b-cfb1-44de-bb7e-5608ed672969"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.657356229869745"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gmg_MC-ZrW6P",
        "outputId": "be381d35-2deb-4a22-95b9-58ae19366943"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rate Optimal Binary Linear Locally Repairable Codes with Small Availability'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst_predictions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Z4tqPwR9vI1T",
        "outputId": "83e85511-1d13-4d18-f9e4-71ca509252fb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"code symbol said to have $(r,t)$-availability if it can be recovered. if rate is maximum, it is said to be 'rate-optimal'\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}