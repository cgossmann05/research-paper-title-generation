{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 111040, done.\u001b[K\n",
      "remote: Counting objects: 100% (277/277), done.\u001b[K\n",
      "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
      "remote: Total 111040 (delta 155), reused 226 (delta 132), pack-reused 110763\u001b[K\n",
      "Receiving objects: 100% (111040/111040), 104.51 MiB | 8.75 MiB/s, done.\n",
      "Resolving deltas: 100% (82396/82396), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: switching to 'v4.21.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at a9eee2ffe Release: v4.21.0\n"
     ]
    }
   ],
   "source": [
    "! cd transformers/ && ! git checkout v4.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.13.1-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets>=1.8.0\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[K     |████████████████████████████████| 441 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from -r transformers/examples/pytorch/summarization/requirements.txt (line 4)) (3.20.1)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "Requirement already satisfied: nltk in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from -r transformers/examples/pytorch/summarization/requirements.txt (line 6)) (3.6.1)\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.0-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.3\n",
      "  Downloading torch-1.12.1-cp38-none-macosx_10_9_x86_64.whl (137.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 137.8 MB 131 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (5.4.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (1.2.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-macosx_10_9_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: packaging in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (20.9)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (1.20.1)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp38-cp38-macosx_10_13_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (359 kB)\n",
      "\u001b[K     |████████████████████████████████| 359 kB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.3->-r transformers/examples/pytorch/summarization/requirements.txt (line 8)) (3.7.4.3)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl (28 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-macosx_10_9_x86_64.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (20.3.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: filelock in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: psutil in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from accelerate->-r transformers/examples/pytorch/summarization/requirements.txt (line 1)) (5.8.0)\n",
      "Requirement already satisfied: regex in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from nltk->-r transformers/examples/pytorch/summarization/requirements.txt (line 6)) (2021.4.4)\n",
      "Requirement already satisfied: click in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from nltk->-r transformers/examples/pytorch/summarization/requirements.txt (line 6)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from nltk->-r transformers/examples/pytorch/summarization/requirements.txt (line 6)) (1.0.1)\n",
      "Collecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp38-cp38-macosx_10_9_x86_64.whl (23 kB)\n",
      "Collecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.15.0-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyppmd<0.19.0,>=0.18.1\n",
      "  Downloading pyppmd-0.18.3-cp38-cp38-macosx_10_9_x86_64.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
      "Collecting inflate64>=0.3.0\n",
      "  Downloading inflate64-0.3.0-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.3-cp38-cp38-macosx_10_9_x86_64.whl (391 kB)\n",
      "\u001b[K     |████████████████████████████████| 391 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-macosx_10_9_x86_64.whl (421 kB)\n",
      "\u001b[K     |████████████████████████████████| 421 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: absl-py in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from rouge-score->-r transformers/examples/pytorch/summarization/requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from rouge-score->-r transformers/examples/pytorch/summarization/requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/summarization/requirements.txt (line 2)) (2021.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24956 sha256=0fcc75cb6b0ecbff580c467382163f63d4e0fafde2dc29d9731840c61f6d9fd5\n",
      "  Stored in directory: /Users/cgossmann/Library/Caches/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: multidict, frozenlist, yarl, charset-normalizer, async-timeout, aiosignal, tqdm, fsspec, dill, aiohttp, xxhash, torch, texttable, responses, pyzstd, pyppmd, pycryptodomex, pybcj, pyarrow, multivolumefile, multiprocess, inflate64, huggingface-hub, brotli, sentencepiece, rouge-score, py7zr, datasets, accelerate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed accelerate-0.13.1 aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 brotli-1.0.9 charset-normalizer-2.1.1 datasets-2.6.1 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2022.8.2 huggingface-hub-0.10.1 inflate64-0.3.0 multidict-6.0.2 multiprocess-0.70.13 multivolumefile-0.2.3 py7zr-0.20.0 pyarrow-9.0.0 pybcj-1.0.1 pycryptodomex-3.15.0 pyppmd-0.18.3 pyzstd-0.15.3 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.97 texttable-1.6.4 torch-1.12.1 tqdm-4.64.1 xxhash-3.0.0 yarl-1.8.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -r transformers/examples/pytorch/summarization/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.21.0\n",
      "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (0.10.1)\n",
      "Requirement already satisfied: requests in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (1.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (2021.4.4)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 45.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.21.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.21.0) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.21.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.21.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.21.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.21.0) (2020.12.5)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.12.1 transformers-4.21.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "10/15/2022 09:19:29 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "10/15/2022 09:19:29 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=tst_summarization/runs/Oct15_09-19-29_MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=tst_summarization,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=tst_summarization,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/15/2022 09:19:30 - WARNING - datasets.builder - Using custom data configuration default-191caab8ee788ed6\n",
      "10/15/2022 09:19:30 - INFO - datasets.info - Loading Dataset Infos from /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "10/15/2022 09:19:30 - INFO - datasets.builder - Generating dataset csv (/Users/cgossmann/.cache/huggingface/datasets/csv/default-191caab8ee788ed6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Downloading and preparing dataset csv/default to /Users/cgossmann/.cache/huggingface/datasets/csv/default-191caab8ee788ed6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 12652.50it/s]\n",
      "10/15/2022 09:19:30 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "10/15/2022 09:19:30 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 714.59it/s]\n",
      "10/15/2022 09:19:30 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "10/15/2022 09:19:30 - INFO - datasets.builder - Generating train split\n",
      "10/15/2022 09:19:30 - INFO - datasets.builder - Generating validation split\n",
      "10/15/2022 09:19:30 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /Users/cgossmann/.cache/huggingface/datasets/csv/default-191caab8ee788ed6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 194.05it/s]\n",
      "[INFO|hub.py:600] 2022-10-15 09:19:30,721 >> https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /Users/cgossmann/.cache/huggingface/transformers/tmp0a_ytbug\n",
      "Downloading config.json: 100%|██████████████| 1.17k/1.17k [00:00<00:00, 247kB/s]\n",
      "[INFO|hub.py:613] 2022-10-15 09:19:30,995 >> storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|hub.py:621] 2022-10-15 09:19:30,995 >> creating metadata file for /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:674] 2022-10-15 09:19:30,997 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-15 09:19:32,467 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:404] 2022-10-15 09:19:32,717 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:674] 2022-10-15 09:19:32,982 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-15 09:19:32,984 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|hub.py:600] 2022-10-15 09:19:33,527 >> https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /Users/cgossmann/.cache/huggingface/transformers/tmp7cbrpit8\n",
      "Downloading spiece.model: 100%|██████████████| 773k/773k [00:00<00:00, 3.29MB/s]\n",
      "[INFO|hub.py:613] 2022-10-15 09:19:34,103 >> storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|hub.py:621] 2022-10-15 09:19:34,104 >> creating metadata file for /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|hub.py:600] 2022-10-15 09:19:34,358 >> https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /Users/cgossmann/.cache/huggingface/transformers/tmprni63ari\n",
      "Downloading tokenizer.json: 100%|██████████| 1.32M/1.32M [00:00<00:00, 3.89MB/s]\n",
      "[INFO|hub.py:613] 2022-10-15 09:19:35,050 >> storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|hub.py:621] 2022-10-15 09:19:35,051 >> creating metadata file for /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-15 09:19:35,869 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-15 09:19:35,870 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-15 09:19:35,870 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-15 09:19:35,870 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-15 09:19:35,870 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:674] 2022-10-15 09:19:36,144 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-15 09:19:36,145 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|hub.py:600] 2022-10-15 09:19:36,562 >> https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/cgossmann/.cache/huggingface/transformers/tmpt9ylh14u\n",
      "Downloading pytorch_model.bin: 100%|█████████| 231M/231M [00:20<00:00, 11.9MB/s]\n",
      "[INFO|hub.py:613] 2022-10-15 09:19:57,140 >> storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|hub.py:621] 2022-10-15 09:19:57,141 >> creating metadata file for /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2034] 2022-10-15 09:19:57,141 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2428] 2022-10-15 09:19:57,702 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2436] 2022-10-15 09:19:57,702 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Running tokenizer on train dataset:   0%|                | 0/17 [00:00<?, ?ba/s]10/15/2022 09:19:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/cgossmann/.cache/huggingface/datasets/csv/default-191caab8ee788ed6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8779f99cb617bd59.arrow\n",
      "Running tokenizer on train dataset:  94%|██████▌| 16/17 [00:04<00:00,  3.41ba/s]\n",
      "Running tokenizer on validation dataset:   0%|            | 0/5 [00:00<?, ?ba/s]10/15/2022 09:20:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/cgossmann/.cache/huggingface/datasets/csv/default-191caab8ee788ed6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-669e9269704119ff.arrow\n",
      "Running tokenizer on validation dataset:  80%|███▏| 4/5 [00:00<00:00,  4.75ba/s]\n",
      "transformers/examples/pytorch/summarization/run_summarization.py:602: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "10/15/2022 09:20:04 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.6.1/metrics/rouge/rouge.py not found in cache or force_download set to True, downloading to /Users/cgossmann/.cache/huggingface/datasets/downloads/tmpf3y9rhl_\n",
      "Downloading builder script: 5.60kB [00:00, 4.30MB/s]                            \n",
      "10/15/2022 09:20:04 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.6.1/metrics/rouge/rouge.py in cache at /Users/cgossmann/.cache/huggingface/datasets/downloads/dac68e38fe6792a70ebadff2418edea646b1abef0f3609c76b7d1dbbcf53a900.e36610c78cc3081112157b0b90933ccd3ec5e07f75121fb9823b3ec2628cf77b.py\n",
      "10/15/2022 09:20:04 - INFO - datasets.utils.file_utils - creating metadata file for /Users/cgossmann/.cache/huggingface/datasets/downloads/dac68e38fe6792a70ebadff2418edea646b1abef0f3609c76b7d1dbbcf53a900.e36610c78cc3081112157b0b90933ccd3ec5e07f75121fb9823b3ec2628cf77b.py\n",
      "/Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1605] 2022-10-15 09:20:04,183 >> ***** Running training *****\n",
      "[INFO|trainer.py:1606] 2022-10-15 09:20:04,183 >>   Num examples = 16777\n",
      "[INFO|trainer.py:1607] 2022-10-15 09:20:04,183 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1608] 2022-10-15 09:20:04,184 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1609] 2022-10-15 09:20:04,184 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1610] 2022-10-15 09:20:04,184 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1611] 2022-10-15 09:20:04,184 >>   Total optimization steps = 12585\n",
      "{'loss': 2.8379, 'learning_rate': 4.801350814461661e-05, 'epoch': 0.12}         \n",
      "  4%|█▍                                   | 500/12585 [24:02<5:56:40,  1.77s/it][INFO|trainer.py:2640] 2022-10-15 09:44:06,268 >> Saving model checkpoint to tst_summarization/checkpoint-500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 09:44:06,273 >> Configuration saved in tst_summarization/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 09:44:06,402 >> Model weights saved in tst_summarization/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 09:44:06,403 >> tokenizer config file saved in tst_summarization/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 09:44:06,403 >> Special tokens file saved in tst_summarization/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 09:44:06,443 >> Copy vocab file to tst_summarization/checkpoint-500/spiece.model\n",
      "{'loss': 2.6181, 'learning_rate': 4.602701628923322e-05, 'epoch': 0.24}         \n",
      "  8%|██▊                                 | 1000/12585 [42:03<6:21:02,  1.97s/it][INFO|trainer.py:2640] 2022-10-15 10:02:08,073 >> Saving model checkpoint to tst_summarization/checkpoint-1000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 10:02:08,076 >> Configuration saved in tst_summarization/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 10:02:08,192 >> Model weights saved in tst_summarization/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 10:02:08,192 >> tokenizer config file saved in tst_summarization/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 10:02:08,193 >> Special tokens file saved in tst_summarization/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 10:02:08,233 >> Copy vocab file to tst_summarization/checkpoint-1000/spiece.model\n",
      "{'loss': 2.5803, 'learning_rate': 4.404052443384982e-05, 'epoch': 0.36}         \n",
      " 12%|████                              | 1500/12585 [1:01:46<6:27:52,  2.10s/it][INFO|trainer.py:2640] 2022-10-15 10:21:50,629 >> Saving model checkpoint to tst_summarization/checkpoint-1500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 10:21:50,634 >> Configuration saved in tst_summarization/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 10:21:50,754 >> Model weights saved in tst_summarization/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 10:21:50,757 >> tokenizer config file saved in tst_summarization/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 10:21:50,757 >> Special tokens file saved in tst_summarization/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 10:21:50,831 >> Copy vocab file to tst_summarization/checkpoint-1500/spiece.model\n",
      "{'loss': 2.534, 'learning_rate': 4.205403257846643e-05, 'epoch': 0.48}          \n",
      " 16%|█████▍                            | 2000/12585 [1:18:58<5:48:25,  1.98s/it][INFO|trainer.py:2640] 2022-10-15 10:39:02,984 >> Saving model checkpoint to tst_summarization/checkpoint-2000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 10:39:02,987 >> Configuration saved in tst_summarization/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 10:39:03,108 >> Model weights saved in tst_summarization/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 10:39:03,111 >> tokenizer config file saved in tst_summarization/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 10:39:03,111 >> Special tokens file saved in tst_summarization/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 10:39:03,171 >> Copy vocab file to tst_summarization/checkpoint-2000/spiece.model\n",
      "{'loss': 2.453, 'learning_rate': 4.0067540723083036e-05, 'epoch': 0.6}          \n",
      " 20%|██████▊                           | 2500/12585 [1:37:07<5:42:22,  2.04s/it][INFO|trainer.py:2640] 2022-10-15 10:57:11,309 >> Saving model checkpoint to tst_summarization/checkpoint-2500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 10:57:11,310 >> Configuration saved in tst_summarization/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 10:57:11,422 >> Model weights saved in tst_summarization/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 10:57:11,422 >> tokenizer config file saved in tst_summarization/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 10:57:11,423 >> Special tokens file saved in tst_summarization/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 10:57:11,449 >> Copy vocab file to tst_summarization/checkpoint-2500/spiece.model\n",
      "{'loss': 2.4911, 'learning_rate': 3.8081048867699645e-05, 'epoch': 0.72}        \n",
      " 24%|████████                          | 3000/12585 [1:55:06<4:32:34,  1.71s/it][INFO|trainer.py:2640] 2022-10-15 11:15:10,945 >> Saving model checkpoint to tst_summarization/checkpoint-3000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 11:15:10,949 >> Configuration saved in tst_summarization/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 11:15:11,061 >> Model weights saved in tst_summarization/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 11:15:11,063 >> tokenizer config file saved in tst_summarization/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 11:15:11,063 >> Special tokens file saved in tst_summarization/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 11:15:11,135 >> Copy vocab file to tst_summarization/checkpoint-3000/spiece.model\n",
      "{'loss': 2.4312, 'learning_rate': 3.6094557012316246e-05, 'epoch': 0.83}        \n",
      " 28%|█████████▍                        | 3500/12585 [2:12:36<5:19:17,  2.11s/it][INFO|trainer.py:2640] 2022-10-15 11:32:40,737 >> Saving model checkpoint to tst_summarization/checkpoint-3500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 11:32:40,738 >> Configuration saved in tst_summarization/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 11:32:40,850 >> Model weights saved in tst_summarization/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 11:32:40,851 >> tokenizer config file saved in tst_summarization/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 11:32:40,851 >> Special tokens file saved in tst_summarization/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 11:32:40,881 >> Copy vocab file to tst_summarization/checkpoint-3500/spiece.model\n",
      "{'loss': 2.4605, 'learning_rate': 3.4108065156932854e-05, 'epoch': 0.95}        \n",
      " 32%|██████████▊                       | 4000/12585 [2:29:56<5:41:55,  2.39s/it][INFO|trainer.py:2640] 2022-10-15 11:50:00,961 >> Saving model checkpoint to tst_summarization/checkpoint-4000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 11:50:00,963 >> Configuration saved in tst_summarization/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 11:50:01,074 >> Model weights saved in tst_summarization/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 11:50:01,075 >> tokenizer config file saved in tst_summarization/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 11:50:01,075 >> Special tokens file saved in tst_summarization/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 11:50:01,100 >> Copy vocab file to tst_summarization/checkpoint-4000/spiece.model\n",
      "{'loss': 2.3909, 'learning_rate': 3.212157330154946e-05, 'epoch': 1.07}         \n",
      " 36%|████████████▏                     | 4500/12585 [2:47:31<5:06:46,  2.28s/it][INFO|trainer.py:2640] 2022-10-15 12:07:36,075 >> Saving model checkpoint to tst_summarization/checkpoint-4500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 12:07:36,076 >> Configuration saved in tst_summarization/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 12:07:36,186 >> Model weights saved in tst_summarization/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 12:07:36,187 >> tokenizer config file saved in tst_summarization/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 12:07:36,187 >> Special tokens file saved in tst_summarization/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 12:07:36,212 >> Copy vocab file to tst_summarization/checkpoint-4500/spiece.model\n",
      "{'loss': 2.3636, 'learning_rate': 3.0135081446166074e-05, 'epoch': 1.19}        \n",
      " 40%|█████████████▌                    | 5000/12585 [3:05:03<4:09:59,  1.98s/it][INFO|trainer.py:2640] 2022-10-15 12:25:07,825 >> Saving model checkpoint to tst_summarization/checkpoint-5000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 12:25:07,826 >> Configuration saved in tst_summarization/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 12:25:07,936 >> Model weights saved in tst_summarization/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 12:25:07,937 >> tokenizer config file saved in tst_summarization/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 12:25:07,937 >> Special tokens file saved in tst_summarization/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 12:25:07,963 >> Copy vocab file to tst_summarization/checkpoint-5000/spiece.model\n",
      "{'loss': 2.3769, 'learning_rate': 2.8148589590782682e-05, 'epoch': 1.31}        \n",
      " 44%|██████████████▊                   | 5500/12585 [3:27:56<4:32:47,  2.31s/it][INFO|trainer.py:2640] 2022-10-15 12:48:00,194 >> Saving model checkpoint to tst_summarization/checkpoint-5500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 12:48:00,197 >> Configuration saved in tst_summarization/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 12:48:00,312 >> Model weights saved in tst_summarization/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 12:48:00,313 >> tokenizer config file saved in tst_summarization/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 12:48:00,313 >> Special tokens file saved in tst_summarization/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 12:48:00,354 >> Copy vocab file to tst_summarization/checkpoint-5500/spiece.model\n",
      "{'loss': 2.3197, 'learning_rate': 2.6162097735399287e-05, 'epoch': 1.43}        \n",
      " 48%|████████████████▏                 | 6000/12585 [3:50:14<4:17:18,  2.34s/it][INFO|trainer.py:2640] 2022-10-15 13:10:18,338 >> Saving model checkpoint to tst_summarization/checkpoint-6000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 13:10:18,341 >> Configuration saved in tst_summarization/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 13:10:18,459 >> Model weights saved in tst_summarization/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 13:10:18,460 >> tokenizer config file saved in tst_summarization/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 13:10:18,460 >> Special tokens file saved in tst_summarization/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 13:10:18,503 >> Copy vocab file to tst_summarization/checkpoint-6000/spiece.model\n",
      "{'loss': 2.3625, 'learning_rate': 2.417560588001589e-05, 'epoch': 1.55}         \n",
      " 52%|█████████████████▌                | 6500/12585 [4:56:26<3:38:30,  2.15s/it][INFO|trainer.py:2640] 2022-10-15 14:16:30,740 >> Saving model checkpoint to tst_summarization/checkpoint-6500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 14:16:30,744 >> Configuration saved in tst_summarization/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 14:16:30,861 >> Model weights saved in tst_summarization/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 14:16:30,862 >> tokenizer config file saved in tst_summarization/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 14:16:30,862 >> Special tokens file saved in tst_summarization/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 14:16:30,900 >> Copy vocab file to tst_summarization/checkpoint-6500/spiece.model\n",
      "{'loss': 2.3533, 'learning_rate': 2.21891140246325e-05, 'epoch': 1.67}          \n",
      " 56%|██████████████████▉               | 7000/12585 [6:37:30<3:42:48,  2.39s/it][INFO|trainer.py:2640] 2022-10-15 15:57:34,849 >> Saving model checkpoint to tst_summarization/checkpoint-7000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 15:57:34,850 >> Configuration saved in tst_summarization/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 15:57:34,978 >> Model weights saved in tst_summarization/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 15:57:34,979 >> tokenizer config file saved in tst_summarization/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 15:57:34,979 >> Special tokens file saved in tst_summarization/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 15:57:35,009 >> Copy vocab file to tst_summarization/checkpoint-7000/spiece.model\n",
      "{'loss': 2.3324, 'learning_rate': 2.0202622169249108e-05, 'epoch': 1.79}        \n",
      " 60%|████████████████████▎             | 7500/12585 [6:56:11<4:48:59,  3.41s/it][INFO|trainer.py:2640] 2022-10-15 16:16:15,867 >> Saving model checkpoint to tst_summarization/checkpoint-7500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 16:16:15,868 >> Configuration saved in tst_summarization/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 16:16:15,999 >> Model weights saved in tst_summarization/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 16:16:16,000 >> tokenizer config file saved in tst_summarization/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 16:16:16,000 >> Special tokens file saved in tst_summarization/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 16:16:16,033 >> Copy vocab file to tst_summarization/checkpoint-7500/spiece.model\n",
      "{'loss': 2.3213, 'learning_rate': 1.8216130313865716e-05, 'epoch': 1.91}        \n",
      " 64%|█████████████████████▌            | 8000/12585 [7:15:50<3:06:59,  2.45s/it][INFO|trainer.py:2640] 2022-10-15 16:35:55,076 >> Saving model checkpoint to tst_summarization/checkpoint-8000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 16:35:55,078 >> Configuration saved in tst_summarization/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 16:35:55,216 >> Model weights saved in tst_summarization/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 16:35:55,217 >> tokenizer config file saved in tst_summarization/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 16:35:55,217 >> Special tokens file saved in tst_summarization/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 16:35:55,251 >> Copy vocab file to tst_summarization/checkpoint-8000/spiece.model\n",
      "{'loss': 2.3247, 'learning_rate': 1.622963845848232e-05, 'epoch': 2.03}         \n",
      " 68%|██████████████████████▉           | 8500/12585 [8:04:03<2:11:19,  1.93s/it][INFO|trainer.py:2640] 2022-10-15 17:24:08,008 >> Saving model checkpoint to tst_summarization/checkpoint-8500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 17:24:08,009 >> Configuration saved in tst_summarization/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 17:24:08,140 >> Model weights saved in tst_summarization/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 17:24:08,141 >> tokenizer config file saved in tst_summarization/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 17:24:08,141 >> Special tokens file saved in tst_summarization/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 17:24:08,173 >> Copy vocab file to tst_summarization/checkpoint-8500/spiece.model\n",
      "{'loss': 2.2782, 'learning_rate': 1.4243146603098927e-05, 'epoch': 2.15}        \n",
      " 72%|████████████████████████▎         | 9000/12585 [8:22:32<2:18:11,  2.31s/it][INFO|trainer.py:2640] 2022-10-15 17:42:36,639 >> Saving model checkpoint to tst_summarization/checkpoint-9000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 17:42:36,640 >> Configuration saved in tst_summarization/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 17:42:36,749 >> Model weights saved in tst_summarization/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 17:42:36,749 >> tokenizer config file saved in tst_summarization/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 17:42:36,750 >> Special tokens file saved in tst_summarization/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 17:42:36,776 >> Copy vocab file to tst_summarization/checkpoint-9000/spiece.model\n",
      "{'loss': 2.2427, 'learning_rate': 1.2256654747715535e-05, 'epoch': 2.26}        \n",
      " 75%|█████████████████████████▋        | 9500/12585 [8:41:24<1:42:26,  1.99s/it][INFO|trainer.py:2640] 2022-10-15 18:01:28,271 >> Saving model checkpoint to tst_summarization/checkpoint-9500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 18:01:28,276 >> Configuration saved in tst_summarization/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 18:01:28,396 >> Model weights saved in tst_summarization/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 18:01:28,399 >> tokenizer config file saved in tst_summarization/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 18:01:28,399 >> Special tokens file saved in tst_summarization/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 18:01:28,474 >> Copy vocab file to tst_summarization/checkpoint-9500/spiece.model\n",
      "{'loss': 2.2975, 'learning_rate': 1.0270162892332142e-05, 'epoch': 2.38}        \n",
      " 79%|██████████████████████████▏      | 10000/12585 [8:59:03<1:35:10,  2.21s/it][INFO|trainer.py:2640] 2022-10-15 18:19:07,449 >> Saving model checkpoint to tst_summarization/checkpoint-10000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 18:19:07,452 >> Configuration saved in tst_summarization/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 18:19:07,570 >> Model weights saved in tst_summarization/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 18:19:07,570 >> tokenizer config file saved in tst_summarization/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 18:19:07,570 >> Special tokens file saved in tst_summarization/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 18:19:07,612 >> Copy vocab file to tst_summarization/checkpoint-10000/spiece.model\n",
      "{'loss': 2.2606, 'learning_rate': 8.28367103694875e-06, 'epoch': 2.5}           \n",
      " 83%|███████████████████████████▌     | 10500/12585 [9:17:30<1:00:50,  1.75s/it][INFO|trainer.py:2640] 2022-10-15 18:37:35,031 >> Saving model checkpoint to tst_summarization/checkpoint-10500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 18:37:35,036 >> Configuration saved in tst_summarization/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 18:37:35,154 >> Model weights saved in tst_summarization/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 18:37:35,157 >> tokenizer config file saved in tst_summarization/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 18:37:35,158 >> Special tokens file saved in tst_summarization/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 18:37:35,233 >> Copy vocab file to tst_summarization/checkpoint-10500/spiece.model\n",
      "{'loss': 2.2813, 'learning_rate': 6.297179181565356e-06, 'epoch': 2.62}         \n",
      " 87%|████████████████████████████▊    | 11000/12585 [9:35:54<1:19:42,  3.02s/it][INFO|trainer.py:2640] 2022-10-15 18:55:58,791 >> Saving model checkpoint to tst_summarization/checkpoint-11000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 18:55:58,794 >> Configuration saved in tst_summarization/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 18:55:58,911 >> Model weights saved in tst_summarization/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 18:55:58,912 >> tokenizer config file saved in tst_summarization/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 18:55:58,912 >> Special tokens file saved in tst_summarization/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 18:55:58,957 >> Copy vocab file to tst_summarization/checkpoint-11000/spiece.model\n",
      "{'loss': 2.2899, 'learning_rate': 4.310687326181963e-06, 'epoch': 2.74}         \n",
      " 91%|████████████████████████████▎  | 11500/12585 [10:42:59<21:30:17, 71.35s/it][INFO|trainer.py:2640] 2022-10-15 20:03:03,923 >> Saving model checkpoint to tst_summarization/checkpoint-11500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 20:03:03,926 >> Configuration saved in tst_summarization/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 20:03:04,039 >> Model weights saved in tst_summarization/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 20:03:04,039 >> tokenizer config file saved in tst_summarization/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 20:03:04,040 >> Special tokens file saved in tst_summarization/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 20:03:04,077 >> Copy vocab file to tst_summarization/checkpoint-11500/spiece.model\n",
      "{'loss': 2.3051, 'learning_rate': 2.32419547079857e-06, 'epoch': 2.86}          \n",
      " 95%|████████████████████████████████▍ | 12000/12585 [11:02:25<19:17,  1.98s/it][INFO|trainer.py:2640] 2022-10-15 20:22:29,847 >> Saving model checkpoint to tst_summarization/checkpoint-12000\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 20:22:29,852 >> Configuration saved in tst_summarization/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 20:22:29,970 >> Model weights saved in tst_summarization/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 20:22:29,971 >> tokenizer config file saved in tst_summarization/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 20:22:29,971 >> Special tokens file saved in tst_summarization/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 20:22:30,011 >> Copy vocab file to tst_summarization/checkpoint-12000/spiece.model\n",
      "{'loss': 2.2654, 'learning_rate': 3.3770361541517685e-07, 'epoch': 2.98}        \n",
      " 99%|█████████████████████████████████▊| 12500/12585 [12:33:58<03:58,  2.81s/it][INFO|trainer.py:2640] 2022-10-15 21:54:03,124 >> Saving model checkpoint to tst_summarization/checkpoint-12500\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 21:54:03,131 >> Configuration saved in tst_summarization/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 21:54:03,255 >> Model weights saved in tst_summarization/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 21:54:03,259 >> tokenizer config file saved in tst_summarization/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 21:54:03,259 >> Special tokens file saved in tst_summarization/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 21:54:03,336 >> Copy vocab file to tst_summarization/checkpoint-12500/spiece.model\n",
      "100%|██████████████████████████████████| 12585/12585 [12:36:56<00:00,  1.50s/it][INFO|trainer.py:1850] 2022-10-15 21:57:00,689 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 45416.5064, 'train_samples_per_second': 1.108, 'train_steps_per_second': 0.277, 'train_loss': 2.3898500850951807, 'epoch': 3.0}\n",
      "100%|██████████████████████████████████| 12585/12585 [12:36:56<00:00,  3.61s/it]\n",
      "[INFO|trainer.py:2640] 2022-10-15 21:57:00,692 >> Saving model checkpoint to tst_summarization\n",
      "[INFO|configuration_utils.py:451] 2022-10-15 21:57:00,694 >> Configuration saved in tst_summarization/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-15 21:57:00,806 >> Model weights saved in tst_summarization/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-15 21:57:00,806 >> tokenizer config file saved in tst_summarization/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-15 21:57:00,806 >> Special tokens file saved in tst_summarization/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-10-15 21:57:00,833 >> Copy vocab file to tst_summarization/spiece.model\n",
      "***** train metrics *****\n",
      "  epoch                    =         3.0\n",
      "  train_loss               =      2.3899\n",
      "  train_runtime            = 12:36:56.50\n",
      "  train_samples            =       16777\n",
      "  train_samples_per_second =       1.108\n",
      "  train_steps_per_second   =       0.277\n",
      "10/15/2022 21:57:00 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:2891] 2022-10-15 21:57:00,875 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2022-10-15 21:57:00,875 >>   Num examples = 4195\n",
      "[INFO|trainer.py:2896] 2022-10-15 21:57:00,876 >>   Batch size = 4\n",
      "100%|█████████████████████████████████████| 1049/1049 [1:54:30<00:00,  1.78s/it]10/15/2022 23:51:37 - INFO - datasets.metric - Removing /Users/cgossmann/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████| 1049/1049 [1:54:34<00:00,  6.55s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_gen_len            =    17.0479\n",
      "  eval_loss               =     2.1401\n",
      "  eval_rouge1             =    40.6688\n",
      "  eval_rouge2             =    21.7479\n",
      "  eval_rougeL             =    36.4917\n",
      "  eval_rougeLsum          =    36.5239\n",
      "  eval_runtime            = 1:54:36.23\n",
      "  eval_samples            =       4195\n",
      "  eval_samples_per_second =       0.61\n",
      "  eval_steps_per_second   =      0.153\n",
      "[INFO|modelcard.py:468] 2022-10-15 23:51:37,613 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 40.6688}]}\n"
     ]
    }
   ],
   "source": [
    "! python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file train_data.csv \\\n",
    "    --validation_file validation_data.csv \\\n",
    "    --text_column abstract \\\n",
    "    --summary_column title \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --output_dir tst_summarization \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "10/23/2022 10:31:26 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "10/23/2022 10:31:26 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=val_summary/runs/Oct23_10-31-25_MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=val_summary,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=val_summary,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/23/2022 10:31:26 - WARNING - datasets.builder - Using custom data configuration default-52b34fb790a49fe7\n",
      "10/23/2022 10:31:26 - INFO - datasets.info - Loading Dataset Infos from /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "10/23/2022 10:31:26 - INFO - datasets.builder - Generating dataset csv (/Users/cgossmann/.cache/huggingface/datasets/csv/default-52b34fb790a49fe7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Downloading and preparing dataset csv/default to /Users/cgossmann/.cache/huggingface/datasets/csv/default-52b34fb790a49fe7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 12576.62it/s]\n",
      "10/23/2022 10:31:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "10/23/2022 10:31:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 979.41it/s]\n",
      "10/23/2022 10:31:26 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "10/23/2022 10:31:26 - INFO - datasets.builder - Generating validation split\n",
      "10/23/2022 10:31:26 - INFO - datasets.builder - Generating test split\n",
      "10/23/2022 10:31:26 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /Users/cgossmann/.cache/huggingface/datasets/csv/default-52b34fb790a49fe7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 283.77it/s]\n",
      "[INFO|configuration_utils.py:674] 2022-10-23 10:31:27,259 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-23 10:31:27,288 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:404] 2022-10-23 10:31:27,698 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:674] 2022-10-23 10:31:28,075 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-23 10:31:28,076 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-23 10:31:30,289 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-23 10:31:30,289 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-23 10:31:30,289 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-23 10:31:30,289 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-23 10:31:30,289 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:674] 2022-10-23 10:31:30,697 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-23 10:31:30,699 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2034] 2022-10-23 10:31:31,151 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2428] 2022-10-23 10:31:31,992 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2436] 2022-10-23 10:31:31,992 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Running tokenizer on prediction dataset:   0%|            | 0/5 [00:00<?, ?ba/s]10/23/2022 10:31:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/cgossmann/.cache/huggingface/datasets/csv/default-52b34fb790a49fe7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e9bbc9c4f69797e0.arrow\n",
      "Running tokenizer on prediction dataset:  80%|███▏| 4/5 [00:01<00:00,  3.76ba/s]\n",
      "transformers/examples/pytorch/summarization/run_summarization.py:602: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "10/23/2022 10:31:34 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:2891] 2022-10-23 10:31:34,129 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2893] 2022-10-23 10:31:34,129 >>   Num examples = 4195\n",
      "[INFO|trainer.py:2896] 2022-10-23 10:31:34,129 >>   Batch size = 4\n",
      "100%|█████████████████████████████████████| 1049/1049 [1:10:19<00:00,  3.32s/it]10/23/2022 11:42:02 - INFO - datasets.metric - Removing /Users/cgossmann/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████| 1049/1049 [1:10:25<00:00,  4.03s/it]\n",
      "***** predict metrics *****\n",
      "  predict_gen_len            =    54.9209\n",
      "  predict_loss               =     4.2237\n",
      "  predict_rouge1             =    22.0129\n",
      "  predict_rouge2             =     8.4288\n",
      "  predict_rougeL             =    17.5656\n",
      "  predict_rougeLsum          =    19.2163\n",
      "  predict_runtime            = 1:10:28.68\n",
      "  predict_samples            =       4195\n",
      "  predict_samples_per_second =      0.992\n",
      "  predict_steps_per_second   =      0.248\n",
      "[INFO|modelcard.py:468] 2022-10-23 11:42:03,298 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "# ! python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "#     --do_predict --model_name_or_path tst_summarization/config.json \\\n",
    "#     --output_dir val_results \\\n",
    "#     --validation_file validation_data.csv\n",
    "! python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_predict \\\n",
    "    --test_file validation_data.csv \\\n",
    "    --validation_file validation_data.csv \\\n",
    "    --text_column abstract \\\n",
    "    --summary_column title \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --output_dir val_summary \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "10/31/2022 20:33:23 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "10/31/2022 20:33:23 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=final_val_summary/runs/Oct31_20-33-22_MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=final_val_summary,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=final_val_summary,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/31/2022 20:33:23 - WARNING - datasets.builder - Using custom data configuration default-618c599ccf95523b\n",
      "10/31/2022 20:33:23 - INFO - datasets.info - Loading Dataset Infos from /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "10/31/2022 20:33:23 - INFO - datasets.builder - Generating dataset csv (/Users/cgossmann/.cache/huggingface/datasets/csv/default-618c599ccf95523b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Downloading and preparing dataset csv/default to /Users/cgossmann/.cache/huggingface/datasets/csv/default-618c599ccf95523b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 12069.94it/s]\n",
      "10/31/2022 20:33:23 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "10/31/2022 20:33:23 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 780.92it/s]\n",
      "10/31/2022 20:33:23 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "10/31/2022 20:33:23 - INFO - datasets.builder - Generating validation split\n",
      "10/31/2022 20:33:23 - INFO - datasets.builder - Generating test split\n",
      "10/31/2022 20:33:23 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /Users/cgossmann/.cache/huggingface/datasets/csv/default-618c599ccf95523b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 317.41it/s]\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 20:33:24,186 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 20:33:24,259 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:404] 2022-10-31 20:33:24,512 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 20:33:24,772 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 20:33:24,774 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 20:33:26,381 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 20:33:26,381 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 20:33:26,381 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 20:33:26,381 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 20:33:26,382 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 20:33:26,658 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 20:33:26,660 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2034] 2022-10-31 20:33:27,065 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2428] 2022-10-31 20:33:27,817 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2436] 2022-10-31 20:33:27,818 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Running tokenizer on prediction dataset:   0%|            | 0/5 [00:00<?, ?ba/s]10/31/2022 20:33:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/cgossmann/.cache/huggingface/datasets/csv/default-618c599ccf95523b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-38e109daa0e1cc3e.arrow\n",
      "Running tokenizer on prediction dataset:  80%|███▏| 4/5 [00:00<00:00,  9.71ba/s]\n",
      "transformers/examples/pytorch/summarization/run_summarization.py:602: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "10/31/2022 20:33:29 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:2891] 2022-10-31 20:33:29,155 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2893] 2022-10-31 20:33:29,155 >>   Num examples = 4195\n",
      "[INFO|trainer.py:2896] 2022-10-31 20:33:29,155 >>   Batch size = 4\n",
      "100%|███████████████████████████████████████| 1049/1049 [23:44<00:00,  1.15s/it]10/31/2022 20:57:20 - INFO - datasets.metric - Removing /Users/cgossmann/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1049/1049 [23:49<00:00,  1.36s/it]\n",
      "***** predict metrics *****\n",
      "  predict_gen_len            =    38.5101\n",
      "  predict_loss               =     5.3289\n",
      "  predict_rouge1             =    21.9953\n",
      "  predict_rouge2             =     7.8976\n",
      "  predict_rougeL             =     17.951\n",
      "  predict_rougeLsum          =    19.0295\n",
      "  predict_runtime            = 0:23:51.35\n",
      "  predict_samples            =       4195\n",
      "  predict_samples_per_second =      2.931\n",
      "  predict_steps_per_second   =      0.733\n",
      "[INFO|modelcard.py:468] 2022-10-31 20:57:20,974 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "! python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_predict \\\n",
    "    --test_file final_test.csv \\\n",
    "    --validation_file final_test.csv \\\n",
    "    --text_column abstract \\\n",
    "    --summary_column title \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --output_dir final_val_summary \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "10/31/2022 21:03:20 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "10/31/2022 21:03:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=final2_val_summary/runs/Oct31_21-03-19_MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=final2_val_summary,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=final2_val_summary,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/31/2022 21:03:20 - WARNING - datasets.builder - Using custom data configuration default-7af1cddbaa553536\n",
      "10/31/2022 21:03:20 - INFO - datasets.info - Loading Dataset Infos from /Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "10/31/2022 21:03:20 - INFO - datasets.builder - Generating dataset csv (/Users/cgossmann/.cache/huggingface/datasets/csv/default-7af1cddbaa553536/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Downloading and preparing dataset csv/default to /Users/cgossmann/.cache/huggingface/datasets/csv/default-7af1cddbaa553536/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 14217.98it/s]\n",
      "10/31/2022 21:03:20 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "10/31/2022 21:03:20 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 882.18it/s]\n",
      "10/31/2022 21:03:20 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "10/31/2022 21:03:20 - INFO - datasets.builder - Generating validation split\n",
      "10/31/2022 21:03:20 - INFO - datasets.builder - Generating test split\n",
      "10/31/2022 21:03:20 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /Users/cgossmann/.cache/huggingface/datasets/csv/default-7af1cddbaa553536/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 358.99it/s]\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 21:03:20,991 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 21:03:21,020 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:404] 2022-10-31 21:03:21,274 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 21:03:21,518 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 21:03:21,519 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 21:03:23,452 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /Users/cgossmann/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 21:03:23,452 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /Users/cgossmann/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 21:03:23,452 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 21:03:23,452 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2022-10-31 21:03:23,452 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:674] 2022-10-31 21:03:23,713 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/cgossmann/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:723] 2022-10-31 21:03:23,715 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/Users/cgossmann/opt/anaconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2034] 2022-10-31 21:03:24,222 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /Users/cgossmann/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2428] 2022-10-31 21:03:24,955 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2436] 2022-10-31 21:03:24,955 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Running tokenizer on prediction dataset:   0%|            | 0/5 [00:00<?, ?ba/s]10/31/2022 21:03:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/cgossmann/.cache/huggingface/datasets/csv/default-7af1cddbaa553536/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9068eff85ed8e828.arrow\n",
      "Running tokenizer on prediction dataset:  80%|███▏| 4/5 [00:00<00:00, 12.00ba/s]\n",
      "transformers/examples/pytorch/summarization/run_summarization.py:602: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "10/31/2022 21:03:26 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:2891] 2022-10-31 21:03:26,037 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2893] 2022-10-31 21:03:26,037 >>   Num examples = 4195\n",
      "[INFO|trainer.py:2896] 2022-10-31 21:03:26,037 >>   Batch size = 4\n",
      "100%|███████████████████████████████████████| 1049/1049 [28:44<00:00,  1.01s/it]10/31/2022 21:32:16 - INFO - datasets.metric - Removing /Users/cgossmann/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1049/1049 [28:48<00:00,  1.65s/it]\n",
      "***** predict metrics *****\n",
      "  predict_gen_len            =    35.1344\n",
      "  predict_loss               =     5.5807\n",
      "  predict_rouge1             =    21.7122\n",
      "  predict_rouge2             =     7.6955\n",
      "  predict_rougeL             =    17.8589\n",
      "  predict_rougeLsum          =    18.7862\n",
      "  predict_runtime            = 0:28:50.71\n",
      "  predict_samples            =       4195\n",
      "  predict_samples_per_second =      2.424\n",
      "  predict_steps_per_second   =      0.606\n",
      "[INFO|modelcard.py:468] 2022-10-31 21:32:17,235 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "! python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_predict \\\n",
    "    --test_file final_test2.csv \\\n",
    "    --validation_file final_test2.csv \\\n",
    "    --text_column abstract \\\n",
    "    --summary_column title \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --output_dir final2_val_summary \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
